import traceback
import time
import re
import os
import random
import pyperclip
import undetected_chromedriver as uc
from selenium.webdriver.common.by import By
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from openai import OpenAI
from config.selectors import GEMINI_CONFIG
from utils.helpers import extract_json_from_text
import json
from selenium.common.exceptions import WebDriverException
# üëá Import c·∫•u h√¨nh
from config.settings import PROFILE_DIR
# üëá ƒê·∫£m b·∫£o b·∫°n ƒë√£ c√≥ h√†m n√†y trong utils (n·∫øu t√™n file kh√°c th√¨ s·ª≠a l·∫°i import)
from utils.helpers import split_srt_blocks 

class VisualPromptGenerator:
    """
    Class chuy√™n tr√°ch nhi·ªám v·ª•: ƒê·ªçc file SRT -> G·ª≠i cho AI -> L·∫•y v·ªÅ JSON Prompt.
    H·ªó tr·ª£: Google Gemini (Web Automation) v√† LM Studio (Local API).
    """

    def __init__(self, status_callback=None):
        """
        Kh·ªüi t·∫°o Generator.
        :param status_callback: H√†m callback(msg) ƒë·ªÉ g·ª≠i log ra giao di·ªán Streamlit.
        """
        self.status_callback = status_callback
        self.driver = None # üëà FIX 1: Gi·ªØ driver ·ªü c·∫•p Class ƒë·ªÉ kh√¥ng b·ªã ng·∫Øt k·∫øt n·ªëi

    def _log(self, msg):
        """H√†m n·ªôi b·ªô ƒë·ªÉ in log ra c·∫£ Terminal v√† UI"""
        print(f"[PromptGen] {msg}")
        if self.status_callback:
            self.status_callback(msg)

    # --- C√ÅC H√ÄM HELPER (Private) ---

    def _focus_window(self):
        """
        üëà FIX 2: H√†m quan tr·ªçng ƒë·ªÉ ƒë∆∞a c·ª≠a s·ªï Chrome l√™n tr√™n c√πng.
        Gi√∫p tr√°nh l·ªói 'Browser window not found' khi paste.
        """
        try:
            if not self.driver or not self.driver.window_handles:
                return False
            
            # L·∫•y handle c·ªßa tab hi·ªán t·∫°i v√† tab ƒë·∫ßu ti√™n
            current = self.driver.current_window_handle
            first = self.driver.window_handles[0]
            
            # N·∫øu ƒëang kh√¥ng ·ªü tab ch√≠nh, chuy·ªÉn v·ªÅ tab ch√≠nh
            if current != first:
                self.driver.switch_to.window(first)
            
            return True
        except Exception:
            # N·∫øu l·ªói focus, c·ª© l·ªù ƒëi ƒë·ªÉ code ch·∫°y ti·∫øp (c√≥ th·ªÉ ng∆∞·ªùi d√πng ƒëang thao t√°c kh√°c)
            return False

    def _wait_for_gemini_finish(self, timeout=120):
        wait = WebDriverWait(self.driver, timeout)
        try:
            # üëá S·ª¨A: L·∫•y selector t·ª´ Config
            wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, GEMINI_CONFIG["SEND_BUTTON"])))
            time.sleep(4) 
            return True
        except Exception:
            return False

    def _count_valid_json_lines(self, text_content):
        if not text_content: return 0
        return len([l for l in text_content.strip().split('\n') if re.search(r'(\{.*\})', l)])

    # üëá H√ÄM KH·ªûI T·∫†O DRIVER RI√äNG (ƒê·ªÇ D·ªÑ RESET)
    def _init_driver(self):
        if not os.path.exists(PROFILE_DIR): os.makedirs(PROFILE_DIR)
        abs_profile_path = os.path.abspath(PROFILE_DIR)
        
        options = uc.ChromeOptions()
        options.add_argument(f'--user-data-dir={abs_profile_path}')
        options.add_argument('--profile-directory=Profile 1')
        options.add_argument('--no-first-run')
        options.add_argument('--start-maximized')
        
        # üëá TH√äM C√ÅC D√íNG N√ÄY ƒê·ªÇ TR√ÅNH CRASH (QUAN TR·ªåNG)
        options.add_argument('--disable-gpu') # T·∫Øt GPU ƒë·ªÉ tr√°nh xung ƒë·ªôt ƒë·ªì h·ªça
        options.add_argument('--no-sandbox')  # Gi·∫£m l·ªói renderer crash
        options.add_argument('--disable-dev-shm-usage') # Tr√°nh l·ªói b·ªô nh·ªõ share
        options.page_load_strategy = 'normal'

        try:
            driver = uc.Chrome(options=options)
            return driver
        except Exception as e:
            self._log(f"‚ùå Kh√¥ng th·ªÉ m·ªü Chrome: {e}")
            return None
    # =========================================================================
    # CH·ª®C NƒÇNG 1: GENERATE QUA GEMINI WEB (FULL OPTIMIZED: JS INJECTION + CONTEXT)
    # =========================================================================
    def generate_via_gemini_web(self, input_srt_path, output_json_path, context_summary="", chunk_size=15, gemini_url=GEMINI_CONFIG["URL"]):
        
        self._log(f"üöÄ Kh·ªüi ƒë·ªông Chrome (Ch·∫ø ƒë·ªô An to√†n)...")
        
        # ƒê√≥ng driver c≈© n·∫øu c√≤n treo
        if self.driver:
            try: self.driver.quit()
            except: pass
        
        # Kh·ªüi t·∫°o driver m·ªõi
        self.driver = self._init_driver()
        if not self.driver: return False

        wait = WebDriverWait(self.driver, 40)
        
        try:
            # 2. CHU·∫®N B·ªä DATA
            blocks = split_srt_blocks(input_srt_path)
            chunks = [blocks[i:i + chunk_size] for i in range(0, len(blocks), chunk_size)]
            self._log(f"üìÑ T·ªïng {len(blocks)} d√≤ng. Chia th√†nh {len(chunks)} chunks (Size={chunk_size}).")
            
            final_data = []

            # 3. PROMPT H·ªÜ TH·ªêNG
            BASE_SYSTEM_PROMPT = f"""
            You are an expert Visual Prompt Creator for AI Video generation.
            Task: Read the subtitle (SRT) lines below and generate a visual illustration description (Visual Prompt) for each line.
            MANDATORY REQUIREMENTS:
            1. Return strictly pure JSON format (Array of Objects).
            2. Each object must follow this structure: {{"index": "keep the original index from input", "text": "original srt content", "visual_prompt": "detailed, artistic image description in English"}}
            3. NO explanations, NO Markdown code blocks, return ONLY the raw JSON string.
            4. ABSOLUTELY DO NOT invent new indices; you must use the exact indices provided in the text.
            DATA TO PROCESS:
            """

            # =================================================
            # V√íNG L·∫∂P CHUNK
            # =================================================
            for index, chunk in enumerate(chunks):
                self._log(f"üîÑ ƒêang x·ª≠ l√Ω Chunk {index + 1}/{len(chunks)}...")
                
                chunk_success = False
                retry_count = 0
                max_retries = 3

                while retry_count < max_retries:
                    try:
                        # KI·ªÇM TRA S·ª∞ S·ªêNG C·ª¶A DRIVER TR∆Ø·ªöC KHI L√ÄM G√å
                        try:
                            # Th·ª≠ g·ªçi m·ªôt l·ªánh nh·∫π ƒë·ªÉ xem Chrome c√≤n s·ªëng kh√¥ng
                            _ = self.driver.window_handles
                        except Exception:
                            raise WebDriverException("Chrome died")

                        # [B∆Ø·ªöC 1] F5 TRANG WEB
                        self.driver.get(gemini_url)
                        
                        # [B∆Ø·ªöC 2] ƒê·ª¢I √î NH·∫¨P LI·ªÜU
                        time.sleep(3) # TƒÉng delay ƒë·ªÉ ·ªïn ƒë·ªãnh
                        prompt_box = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, GEMINI_CONFIG["INPUT_BOX"])))
                        
                        # [B∆Ø·ªöC 3] JS INJECTION
                        chunk_text = "\n".join(chunk)
                        full_message = f"{BASE_SYSTEM_PROMPT}\n\n{chunk_text}"
                        
                        self._log(f"üìù ƒêang g·ª≠i d·ªØ li·ªáu (JS Injection)...")
                        prompt_box.click()
                        time.sleep(1)

                        self.driver.execute_script(
                            """
                            var elm = arguments[0];
                            elm.focus();
                            document.execCommand('insertText', false, arguments[1]);
                            elm.dispatchEvent(new Event('input', { bubbles: true }));
                            """, 
                            prompt_box, 
                            full_message
                        )
                        time.sleep(1.5)

                        # [B∆Ø·ªöC 4] G·ª¨I
                        try:
                            send_btn = wait.until(EC.element_to_be_clickable((By.CSS_SELECTOR, GEMINI_CONFIG["SEND_BUTTON"])))
                            send_btn.click()
                        except:
                            self._log("‚ö†Ô∏è Kh√¥ng th·∫•y n√∫t G·ª≠i, d√πng Enter...")
                            prompt_box.send_keys(Keys.ENTER)
                        
                        self._log(f"‚è≥ ƒêang ƒë·ª£i AI tr·∫£ l·ªùi (L·∫ßn th·ª≠ {retry_count + 1})...")
                        
                        # [B∆Ø·ªöC 5] L·∫§Y K·∫æT QU·∫¢
                        if self._wait_for_gemini_finish(timeout=90):
                            responses = self.driver.find_elements(By.CSS_SELECTOR, GEMINI_CONFIG["RESPONSE_TEXT"])
                            if responses:
                                latest_response = responses[-1].text
                                parsed_objects = extract_json_from_text(latest_response)
                                
                                expected_count = len(chunk)
                                current_valid = len(parsed_objects)

                                if current_valid >= expected_count:
                                    self._log(f"‚úÖ Chunk {index + 1} th√†nh c√¥ng: {current_valid}/{expected_count} items.")
                                    final_data.extend(parsed_objects)
                                    chunk_success = True
                                    break 
                                else:
                                    # Logic ƒë·ª£i th√™m n·∫øu AI g√µ ch∆∞a xong (tr√°nh l·ªói 0/15)
                                    if current_valid == 0:
                                        self._log("‚ö†Ô∏è Ch∆∞a c√≥ JSON, ƒë·ª£i th√™m 5s...")
                                        time.sleep(5)
                                        # L·∫•y l·∫°i l·∫ßn n·ªØa
                                        responses = self.driver.find_elements(By.CSS_SELECTOR, GEMINI_CONFIG["RESPONSE_TEXT"])
                                        latest_response = responses[-1].text
                                        parsed_objects = extract_json_from_text(latest_response)
                                        current_valid = len(parsed_objects)
                                        if current_valid >= expected_count:
                                            self._log(f"‚úÖ ƒê√£ l·∫•y ƒë∆∞·ª£c ƒë·ªß JSON: {current_valid}.")
                                            final_data.extend(parsed_objects)
                                            chunk_success = True
                                            break

                                    self._log(f"‚ö†Ô∏è Thi·∫øu data ({current_valid}/{expected_count}). Th·ª≠ l·∫°i...")
                            else:
                                self._log("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y ph·∫£n h·ªìi.")
                        else:
                            self._log("‚ö†Ô∏è Timeout.")

                    except (WebDriverException, ConnectionError) as e:
                        # üëá C∆† CH·∫æ H·ªíI SINH TR√åNH DUY·ªÜT T·∫†I ƒê√ÇY
                        self._log(f"üî• C·∫¢NH B√ÅO: Tr√¨nh duy·ªát b·ªã S·∫≠p/Ng·∫Øt k·∫øt n·ªëi! ({str(e)[:50]}...)")
                        self._log("üöë ƒêang H·ªíI SINH tr√¨nh duy·ªát m·ªõi...")
                        
                        try:
                            self.driver.quit()
                        except: pass
                        
                        time.sleep(2)
                        self.driver = self._init_driver() # M·ªü l·∫°i c√°i m·ªõi
                        wait = WebDriverWait(self.driver, 40)
                        
                        if not self.driver:
                            self._log("‚ùå Kh√¥ng th·ªÉ h·ªìi sinh driver. D·ª´ng tool.")
                            return False
                        
                        self._log("‚úÖ ƒê√£ h·ªìi sinh xong. S·∫Ω th·ª≠ l·∫°i chunk n√†y ngay.")
                        # Kh√¥ng tƒÉng retry_count ƒë·ªÉ n√≥ th·ª≠ l·∫°i chunk n√†y v·ªõi tr√¨nh duy·ªát m·ªõi

                    except Exception as e:
                         self._log(f"‚ö†Ô∏è L·ªói logic th∆∞·ªùng: {e}")
                         retry_count += 1
                         time.sleep(2)
                    
                    if not chunk_success and isinstance(self.driver, type(None)) == False:
                        retry_count += 1
                        time.sleep(3)

                if not chunk_success:
                    self._log(f"‚ùå Th·∫•t b·∫°i Chunk {index + 1}. D·ªØ li·ªáu ph·∫ßn n√†y s·∫Ω b·ªã thi·∫øu.")
                
                time.sleep(2)

            # 4. L∆ØU FILE
            self._log(f"üíæ ƒêang l∆∞u {len(final_data)} d√≤ng d·ªØ li·ªáu v√†o file...")
            with open(output_json_path, "w", encoding="utf-8") as f:
                json.dump(final_data, f, ensure_ascii=False, indent=4)
            
            self._log(f"üéâ Ho√†n t·∫•t! File l∆∞u t·∫°i: {output_json_path}")
            return True

        except Exception as e:
            self._log(f"‚ùå L·ªói Critical: {str(e)}")
            traceback.print_exc()
            return False
        finally:
            if self.driver:
                try: self.driver.quit()
                except: pass
                self.driver = None
    # =========================================================================
    # CH·ª®C NƒÇNG 2: GENERATE QUA LOCAL API (LM STUDIO)
    # =========================================================================
    def generate_via_local_api(self, input_srt_path, output_txt_path, chunk_size, api_base, api_key, model_name, system_prompt):
        """
        Ch·∫°y API ƒë·ªÉ l·∫•y prompt t·ª´ LM Studio ho·∫∑c OpenAI Compatible Server.
        """
        try:
            client = OpenAI(base_url=api_base, api_key=api_key)
            
            self._log(f"üîó ƒêang k·∫øt n·ªëi t·ªõi API: {api_base}...")
            try:
                client.models.list()
            except Exception as e:
                self._log(f"‚ùå Kh√¥ng th·ªÉ k·∫øt n·ªëi t·ªõi Server. L·ªói: {e}")
                return False

            blocks = split_srt_blocks(input_srt_path)
            chunks = [blocks[i:i + chunk_size] for i in range(0, len(blocks), chunk_size)]
            self._log(f"üìÑ T·ªïng {len(blocks)} blocks. Chia th√†nh {len(chunks)} chunks.")

            if os.path.exists(output_txt_path): os.remove(output_txt_path)

            with open(output_txt_path, "a", encoding="utf-8") as f_out:
                for index, chunk in enumerate(chunks):
                    user_content = "\n\n".join(chunk)
                    self._log(f"üì§ ƒêang g·ª≠i Chunk {index + 1}/{len(chunks)} qua API...")

                    try:
                        response = client.chat.completions.create(
                            model=model_name,
                            messages=[
                                {"role": "system", "content": system_prompt},
                                {"role": "user", "content": user_content}
                            ],
                            temperature=0.7,
                            stream=False
                        )
                        
                        content = response.choices[0].message.content
                        valid_lines = self._count_valid_json_lines(content)
                        self._log(f"‚úÖ Chunk {index + 1} xong. Nh·∫≠n ƒë∆∞·ª£c {valid_lines} d√≤ng JSON.")
                        
                        f_out.write(content + "\n\n")
                        f_out.flush() 

                    except Exception as e:
                        error_msg = str(e)
                        self._log(f"‚ùå L·ªói API ·ªü Chunk {index + 1}: {error_msg}")
                        f_out.write(f"\n[ERROR CHUNK {index+1}]: {error_msg}\n\n")

            self._log("üéâ Ho√†n t·∫•t x·ª≠ l√Ω qua API!")
            return True

        except Exception as e:
            self._log(f"‚ùå L·ªói kh·ªüi t·∫°o Client: {str(e)}")
            return False